<!DOCTYPE html>





<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 3.9.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.4.0">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.4.0">
  <link rel="mask-icon" href="/images/logo.svg?v=7.4.0" color="#222">

<link rel="stylesheet" href="/css/main.css?v=7.4.0">


<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.7.0">
  <link rel="stylesheet" href="/lib/pace/pace-theme-corner-indicator.min.css?v=1.0.2">
  <script src="/lib/pace/pace.min.js?v=1.0.2"></script>


<script id="hexo-configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Pisces',
    version: '7.4.0',
    exturl: false,
    sidebar: {"position":"left","display":"post","offset":12,"onmobile":false},
    copycode: {"enable":false,"show_result":false,"style":null},
    back2top: {"enable":true,"sidebar":true,"scrollpercent":true},
    bookmark: {"enable":false,"color":"#222","save":"auto"},
    fancybox: false,
    mediumzoom: false,
    lazyload: false,
    pangu: false,
    algolia: {
      appID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    },
    localsearch: {"enable":true,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},
    path: 'search.xml',
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    translation: {
      copy_button: '复制',
      copy_success: '复制成功',
      copy_failure: '复制失败'
    },
    sidebarPadding: 40
  };
</script>

  <meta name="description" content="爬虫是什么爬虫是一个程序，能够拿到信息，可见即可爬先爬一章-一本-全站编译器安装b站视频安装，公众号安装包D:\python\ (b2)pycharm安装，python环境搭建 -&amp;gt;安装包D:\Python3.7.2 -&amp;gt;python自己D:\PyCharm2019\PyCharm Community Edition 2019.1.3 -&amp;gt;pycharm编译器D:\PyCharm">
<meta property="og:type" content="article">
<meta property="og:title" content="python爬虫小说">
<meta property="og:url" content="https://lucifinil-x.github.io/2020/09/03/python爬虫小说/index.html">
<meta property="og:site_name" content="Sanctuary">
<meta property="og:description" content="爬虫是什么爬虫是一个程序，能够拿到信息，可见即可爬先爬一章-一本-全站编译器安装b站视频安装，公众号安装包D:\python\ (b2)pycharm安装，python环境搭建 -&amp;gt;安装包D:\Python3.7.2 -&amp;gt;python自己D:\PyCharm2019\PyCharm Community Edition 2019.1.3 -&amp;gt;pycharm编译器D:\PyCharm">
<meta property="og:locale" content="zh-CN">
<meta property="og:updated_time" content="2021-02-18T13:44:57.665Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="python爬虫小说">
<meta name="twitter:description" content="爬虫是什么爬虫是一个程序，能够拿到信息，可见即可爬先爬一章-一本-全站编译器安装b站视频安装，公众号安装包D:\python\ (b2)pycharm安装，python环境搭建 -&amp;gt;安装包D:\Python3.7.2 -&amp;gt;python自己D:\PyCharm2019\PyCharm Community Edition 2019.1.3 -&amp;gt;pycharm编译器D:\PyCharm">
  <link rel="canonical" href="https://lucifinil-x.github.io/2020/09/03/python爬虫小说/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome: false,
    isPost: true,
    isPage: false,
    isArchive: false
  };
</script>

  <title>python爬虫小说 | Sanctuary</title>
  








  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="zh-CN">
  <div class="container use-motion">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-meta">

    <div>
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">Sanctuary</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
  </div>

  <div class="site-nav-toggle">
    <button aria-label="切换导航栏">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>


<nav class="site-nav">
  
  <ul id="menu" class="menu">
      
      
      
        
        <li class="menu-item menu-item-home">
      
    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>首页</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-tags">
      
    

    <a href="/tags/" rel="section"><i class="menu-item-icon fa fa-fw fa-tags"></i> <br>标签</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-categories">
      
    

    <a href="/categories/" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>分类</a>

  </li>
      
      
      
        
        <li class="menu-item menu-item-archives">
      
    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>归档</a>

  </li>
      <li class="menu-item menu-item-search">
        <a href="javascript:;" class="popup-trigger">
        
          <i class="menu-item-icon fa fa-search fa-fw"></i> <br>搜索</a>
      </li>
    
  </ul>

</nav>
  <div class="site-search">
    <div class="popup search-popup">
    <div class="search-header">
  <span class="search-icon">
    <i class="fa fa-search"></i>
  </span>
  <div class="search-input-container">
    <input autocomplete="off" autocorrect="off" autocapitalize="none"
           placeholder="搜索..." spellcheck="false"
           type="text" id="search-input">
  </div>
  <span class="popup-btn-close">
    <i class="fa fa-times-circle"></i>
  </span>
</div>
<div id="search-result"></div>

</div>
<div class="search-pop-overlay"></div>

  </div>
</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
            

          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
      <article itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block post">
    <link itemprop="mainEntityOfPage" href="https://lucifinil-x.github.io/2020/09/03/python爬虫小说/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="Lucifinil">
      <meta itemprop="description" content="">
      <meta itemprop="image" content="/images/touxiang1.jpg">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="Sanctuary">
    </span>
      <header class="post-header">
        <h1 class="post-title" itemprop="name headline">python爬虫小说

          
        </h1>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              
                
              

              <time title="创建时间：2020-09-03 23:37:45" itemprop="dateCreated datePublished" datetime="2020-09-03T23:37:45+08:00">2020-09-03</time>
            </span>
          
            

            
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-02-18 21:44:57" itemprop="dateModified" datetime="2021-02-18T21:44:57+08:00">2021-02-18</time>
              </span>
            
          

          
            <span class="post-meta-item" title="阅读次数">
              <span class="post-meta-item-icon">
                <i class="fa fa-eye"></i>
              </span>
              <span class="post-meta-item-text">阅读次数：</span>
              <span class="busuanzi-value" id="busuanzi_value_page_pv"></span>
            </span>
          
  
  <span class="post-meta-item">
    
      <span class="post-meta-item-icon">
        <i class="fa fa-comment-o"></i>
      </span>
        
      
      <span class="post-meta-item-text">Valine：</span>
    
    <a title="valine" href="/2020/09/03/python爬虫小说/#comments" itemprop="discussionUrl"><span class="post-comments-count valine-comment-count" data-xid="/2020/09/03/python爬虫小说/" itemprop="commentCount"></span></a>
  </span>
  
  

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
        <h3 id="爬虫是什么"><a href="#爬虫是什么" class="headerlink" title="爬虫是什么"></a>爬虫是什么</h3><p>爬虫是一个程序，能够拿到信息，可见即可爬<br>先爬一章-一本-全站</p><h3 id="编译器安装"><a href="#编译器安装" class="headerlink" title="编译器安装"></a>编译器安装</h3><p>b站视频安装，公众号安装包<br>D:\python\ (b2)pycharm安装，python环境搭建 -&gt;安装包<br>D:\Python3.7.2 -&gt;python自己<br>D:\PyCharm2019\PyCharm Community Edition 2019.1.3 -&gt;pycharm编译器<br>D:\PyCharm2019\Python Project -&gt;py项目仓库</p><a id="more"></a>

<h3 id="用css选择器爬一章小说"><a href="#用css选择器爬一章小说" class="headerlink" title="用css选择器爬一章小说"></a>用css选择器爬一章小说</h3><h4 id="需要的函数库"><a href="#需要的函数库" class="headerlink" title="需要的函数库"></a>需要的函数库</h4><ul>
<li>import requests<br>找不到这个库，需要在setting里安装：project interpreter 点+号查找<br><a href="https://blog.csdn.net/qq_41838901/article/details/89885475" target="_blank" rel="noopener">https://blog.csdn.net/qq_41838901/article/details/89885475</a></li>
</ul>
<p>requests是http请求库， 互联网建立在http协议上</p>
<ul>
<li><p>审查元素-networking-all-ctrl+r-点开一行-点header-request method查看请求方式：get/post</p>
</li>
<li><p>url统一资源定位符 相当于门牌号，根据url就可以请求到内容</p>
</li>
<li><p>from pyquery import PyQuery网页解析器(css解析)<br>需要在setting里安装</p>
</li>
</ul>
<h4 id="封装爬一本小说一章节的方法"><a href="#封装爬一本小说一章节的方法" class="headerlink" title="封装爬一本小说一章节的方法"></a>封装爬一本小说一章节的方法</h4><p>def get_one_chapter(chapter_url=None, name=None):</p>
<h4 id="ip池"><a href="#ip池" class="headerlink" title="ip池"></a>ip池</h4><p>57分钟</p>
<h4 id="爬一整本-反叛-问题"><a href="#爬一整本-反叛-问题" class="headerlink" title="爬一整本(反叛)问题"></a>爬一整本(反叛)问题</h4><p>两个函数都没问题，爬一本的时候出不来。<br>因为获取的links里第一条不是url<br>所以改变获取url的css选择器(直接在标题那个地方右键复制选择器)</p>
<p>但是前面两个还是没有用<br>反叛的大魔王 <a href="https://www.liuzhishu.com/" target="_blank" rel="noopener">https://www.liuzhishu.com/</a><br>反叛的大魔王 <a href="https://www.liuzhishu.com/book22983/" target="_blank" rel="noopener">https://www.liuzhishu.com/book22983/</a><br>反叛的大魔王 <a href="https://www.liuzhishu.com/book22983/4135499.html" target="_blank" rel="noopener">https://www.liuzhishu.com/book22983/4135499.html</a><br>反叛的大魔王 <a href="https://www.liuzhishu.com/book22983/4135501.html" target="_blank" rel="noopener">https://www.liuzhishu.com/book22983/4135501.html</a></p>
<p>所以要去掉前面的链接<br>for link in links.items(): # 这个links里包含前面两个链接</p>
<h1 id="下面这个进行强制类型转换成列表，去掉前面两个"><a href="#下面这个进行强制类型转换成列表，去掉前面两个" class="headerlink" title="下面这个进行强制类型转换成列表，去掉前面两个"></a>下面这个进行强制类型转换成列表，去掉前面两个</h1><p>for link in list(links.items())[2:]:</p>
<h4 id="爬全站"><a href="#爬全站" class="headerlink" title="爬全站"></a>爬全站</h4><p>1：20</p>
<h4 id="爬一本练习-被爱幻想"><a href="#爬一本练习-被爱幻想" class="headerlink" title="爬一本练习(被爱幻想)"></a>爬一本练习(被爱幻想)</h4><ul>
<li>目录 <a href="https://m.ruochenwx.com/131382/" target="_blank" rel="noopener">https://m.ruochenwx.com/131382/</a><br>审查元素-networking-all-ctrl+r<br>点击本页面html,Request URL: <a href="https://m.ruochenwx.com/131382/" target="_blank" rel="noopener">https://m.ruochenwx.com/131382/</a> Request Method: GET</li>
<li>改def get_index():<br>1.目录参数 index_url<br>2.每章超链接选择器参数 links = doc(‘ul.read a’)<br>3.书名选择器参数 name = doc(‘p.name’).text()<br>4.遍历列表的去除数字 for link in list(links.items())[1:]:<br>5.补全每章链接 /131382/21107531.html -&gt; <a href="https://m.ruochenwx.com/131382/21107531.html" target="_blank" rel="noopener">https://m.ruochenwx.com/131382/21107531.html</a></li>
<li>改def get_one_chapter(chapter_url=None, name=None):<br>1.每行选择器参数 content = doc(‘div.content p’).text()<br>2.每章名字选择器参数 title = doc(‘.headline’).text()</li>
<li>问题：<br>目录上每章只有一个html,但是每章有分页，必须找到每章的所有链接</li>
<li>编程思路：</li>
</ul>
<ol>
<li><p>寻找分页地址的变动规律</p>
</li>
<li><p>解析网页，获取内容，放入自定义函数中</p>
</li>
<li><p>调用函数，输出分页内容</p>
</li>
</ol>
<h3 id="用css选择器爬囚徒-有分页，自己分段"><a href="#用css选择器爬囚徒-有分页，自己分段" class="headerlink" title="用css选择器爬囚徒(有分页，自己分段)"></a>用css选择器爬囚徒(有分页，自己分段)</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from pyquery import PyQuery</span><br><span class="line">import re</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_one_chapter(chapter_url=None, name=None):</span><br><span class="line">    response = requests.get(url=chapter_url)</span><br><span class="line">    doc = PyQuery(response.text)</span><br><span class="line">    # body &gt; div.article_title &gt; div.main &gt; div.chapter p</span><br><span class="line">    contentAll = doc(&apos;div.chapter p&apos;).text() # 所有p标签里的内容都在contentAll</span><br><span class="line">    contentLines = contentAll.split(&quot; &quot;) #把contentAll里分段存到数组contentLines</span><br><span class="line">    # #chapter &gt; h1 class=headline</span><br><span class="line">    title = doc(&apos;div.article_title &gt; h1&apos;).text()</span><br><span class="line">    with open(file=name + &quot;.txt&quot;, mode=&quot;a+&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        f.write(title + &quot;\n\n&quot;)</span><br><span class="line">        for contentLine in contentLines: # forin循环取出数组中的每一行 后面加\n</span><br><span class="line">            contentLine += &quot;\n&quot;</span><br><span class="line">            f.write(contentLine)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_index():</span><br><span class="line">    index_url = &apos;http://www.77nt.win/74717/&apos;</span><br><span class="line"></span><br><span class="line">    text = requests.get(url=index_url).text</span><br><span class="line">    doc = PyQuery(text)</span><br><span class="line">    links = doc(&apos;dl a&apos;) # dl标签 a后代</span><br><span class="line">    name = &apos;囚徒&apos;</span><br><span class="line"></span><br><span class="line">    # http://www.77nt.win/74717/1.html 章节第一页</span><br><span class="line">    # http://www.77nt.win/74717/1-2.html 章节第二页</span><br><span class="line"></span><br><span class="line">    for link in list(links.items())[9:]:</span><br><span class="line">        strXuanzeqiGet = link.attr.href # /74717/1.html</span><br><span class="line">        strinfo = re.compile(&apos;.html&apos;)</span><br><span class="line">       # print(strinfo)</span><br><span class="line">        b = strinfo.sub(&apos;&apos;, strXuanzeqiGet) # strXuanzeqiGet去掉&apos;.html&apos;</span><br><span class="line">        # b= /74717/1</span><br><span class="line">        urls = [&apos;.html&apos;, &apos;-2.html&apos;] #每一章的几页的url /74717/1 之后的 数组</span><br><span class="line">        for url in urls:</span><br><span class="line">            #chapter_url = &quot;http://www.77nt.win&quot; + /74717/1 + .html</span><br><span class="line">            #chapter_url = &quot;http://www.77nt.win&quot; + /74717/1 + -2.html</span><br><span class="line">            chapter_url = &quot;http://www.77nt.win&quot; + b + url</span><br><span class="line">            print(name, chapter_url)  # 囚徒 /74717/1.html 所以要加上http://www.77nt.win</span><br><span class="line">            get_one_chapter(chapter_url=chapter_url, name=name)</span><br><span class="line"></span><br><span class="line">get_index()</span><br></pre></td></tr></table></figure>

<h3 id="用xpath爬狱警与黑老大"><a href="#用xpath爬狱警与黑老大" class="headerlink" title="用xpath爬狱警与黑老大"></a>用xpath爬狱警与黑老大</h3><p>开始的乱码要删掉啊，不然运行多少次都不会覆盖上的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from pyquery import PyQuery</span><br><span class="line">import re</span><br><span class="line">from lxml import etree</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_one_chapter(chapter_url=None, name=None):</span><br><span class="line">    response = requests.get(url=chapter_url)</span><br><span class="line">    response.encoding = &quot;utf-8&quot;</span><br><span class="line"></span><br><span class="line">    res_xpath = etree.HTML(response.text)</span><br><span class="line">    content = res_xpath.xpath(&apos;//*[@id=&quot;chaptercontent&quot;]/text()&apos;)</span><br><span class="line">    title = res_xpath.xpath(&apos;//title/text()&apos;)</span><br><span class="line">    with open(file=name + &quot;.txt&quot;, mode=&quot;a+&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        w = str(title[0]) + &quot;\n&quot;</span><br><span class="line">        for con in content:</span><br><span class="line">            w += str(con)</span><br><span class="line">            w += &apos;\n&apos;</span><br><span class="line">        f.write(w)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">def get_index():</span><br><span class="line">    index_url = &apos;http://m.qiliuxs.com/29_29125/all.html&apos;</span><br><span class="line"></span><br><span class="line">    text = requests.get(url=index_url).text</span><br><span class="line">    doc = PyQuery(text)</span><br><span class="line">    # #chapterlist &gt; p:nth-child(2) &gt; a</span><br><span class="line">    links = doc(&apos;#chapterlist a&apos;)</span><br><span class="line">    # &lt;title&gt;武林豪杰皆我奴-v文章节目录,武林豪杰皆我奴-v文最新章节全集_7766小说网手机版&lt;/title&gt;</span><br><span class="line">    name = &apos;武林豪杰皆我奴&apos;</span><br><span class="line"></span><br><span class="line">    for link in list(links.items())[1:]:</span><br><span class="line">        str1 = link.attr.href</span><br><span class="line">        strinfo = re.compile(&apos;.html&apos;)</span><br><span class="line">        b = strinfo.sub(&apos;&apos;, str1)</span><br><span class="line">        print(b)</span><br><span class="line">        urls = [&apos;_&#123;&#125;.html&apos;.format(str(i)) for i in range(1, 4)]</span><br><span class="line"></span><br><span class="line">        for url in urls:</span><br><span class="line">            chapter_url = &apos;http://m.qiliuxs.com&apos; + b + url</span><br><span class="line">            print(name, chapter_url)</span><br><span class="line">            get_one_chapter(chapter_url=chapter_url, name=name)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">get_index()</span><br></pre></td></tr></table></figure>

<h3 id="用css选择器爬御宅屋"><a href="#用css选择器爬御宅屋" class="headerlink" title="用css选择器爬御宅屋"></a>用css选择器爬御宅屋</h3><p>特点：总目录页分页，每章分页</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">from pyquery import PyQuery</span><br><span class="line">import re</span><br><span class="line"># 关键字参数，指定了None,调用函数时候这两个变量就可以互换位置。不写None的话调用函数必须准守参数位置</span><br><span class="line"># chapter_url章节url  name书名</span><br><span class="line">def get_one_chapter(chapter_url=None, name=None):</span><br><span class="line">    response = requests.get(url=chapter_url)</span><br><span class="line">    doc = PyQuery(response.text) # 拿到的text进行解析</span><br><span class="line">    content = doc(&apos;body p&apos;).text() # 拿到.article里的内容</span><br><span class="line">    title = doc(&apos;h1&apos;).text()    # #nr_title</span><br><span class="line">    contentLines = content.split(&quot; &quot;)  # 把contentAll里分段存到数组contentLines</span><br><span class="line"></span><br><span class="line">    with open(file=name + &quot;.txt&quot;, mode=&quot;a+&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        f.write(title + &quot;\n\n&quot;)</span><br><span class="line">        for contentLine in contentLines:  # forin循环取出数组中的每一行 后面加\n</span><br><span class="line">            contentLine += &quot;\n&quot;</span><br><span class="line">            f.write(contentLine)</span><br><span class="line"></span><br><span class="line"># 获取一本书所有章节的url+书名，把拿到的每章的链接丢到上面get_one_chapter函数中就能拿到所有的章内容</span><br><span class="line">def get_index(index_url=None):</span><br><span class="line">    # 目录</span><br><span class="line">    text = requests.get(url=index_url).text</span><br><span class="line">    doc = PyQuery(text) # PyQuery(text)解析</span><br><span class="line">    links = doc(&apos;li a&apos;) # #jsList1 &gt; li:nth-child(1) &gt; a 用li标签的后代 a</span><br><span class="line">    # name = doc(&apos;h1&apos;).text()</span><br><span class="line">    name = &quot;斯德哥尔摩by花臂熊猫&quot;</span><br><span class="line">    for link in list(links.items()):</span><br><span class="line">        chapter_url = link.attr.href  # 提取出a标签中的href属性(链接)/read/69571/7502147.html</span><br><span class="line">        # 每一章里有分页 两个分页。目标：/read/69571/7502147.html -&gt; /read/69571/7502148_1.html /read/69571/7502148_2.html</span><br><span class="line">        reHtml = re.compile(&apos;.html&apos;)</span><br><span class="line">        after_re = reHtml.sub(&apos;&apos;, chapter_url) #after_re = /read/69571/7502147</span><br><span class="line">        add_urls = [&apos;_1.html&apos;, &apos;_2.html&apos;]</span><br><span class="line">        for add_url in add_urls:</span><br><span class="line">            print(&apos;https://yzwbl.com&apos;+after_re+add_url)</span><br><span class="line">            get_one_chapter(chapter_url=&apos;https://yzwbl.com&apos;+after_re+add_url, name=name)</span><br><span class="line"></span><br><span class="line"># 目录页https://yzwbl.com/indexlist/69571/0/ 到 https://yzwbl.com/indexlist/69571/6/</span><br><span class="line"># 写入文件是追加模式，这里已经有0了 从1到6运行 range(1,7)</span><br><span class="line">def main():</span><br><span class="line">    index_urls_list = []</span><br><span class="line">    for num in range(1,7):</span><br><span class="line">        index_urls_list.append(&apos;https://yzwbl.com/indexlist/69571/&apos; + str(num) + &apos;/&apos;)</span><br><span class="line">    print(index_urls_list)</span><br><span class="line">    for index_url in index_urls_list:</span><br><span class="line">        get_index(index_url=index_url)</span><br><span class="line">main()</span><br></pre></td></tr></table></figure>

<h4 id="做两次跳转的函数"><a href="#做两次跳转的函数" class="headerlink" title="做两次跳转的函数"></a>做两次跳转的函数</h4><p>这个网站需要搜索关键词然后爬，这个链接是POST<a href="http://m.qiliuxs.com/s.php" target="_blank" rel="noopener">http://m.qiliuxs.com/s.php</a></p>
<h3 id="用-text-抓取下来的文字乱码"><a href="#用-text-抓取下来的文字乱码" class="headerlink" title="用.text()抓取下来的文字乱码"></a>用.text()抓取下来的文字乱码</h3><p><a href="https://www.zhihu.com/question/39432369" target="_blank" rel="noopener">https://www.zhihu.com/question/39432369</a><br>1.检查网页用的字符集</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># Requests 推测的文本编码（也就是网页返回即爬取下来后的编码转换）与源网页编码不一致，由此可知其正是导致乱码原因。</span><br><span class="line">print(res.encoding)  #查看网页返回的字符集类型 ISO-8859-1</span><br><span class="line">print(res.apparent_encoding) #自动判断字符集类型 utf-8</span><br></pre></td></tr></table></figure>

<p>当源网页编码和爬取下来后的编码转换不一致时，如源网页为gbk编码的字节流，而我们抓取下后程序直接使用utf-8进行编码并输出到存储文件中，这必然会引起乱码，即当源网页编码和抓取下来后程序直接使用处理编码一致时，则不会出现乱码，此时再进行统一的字符编码也就不会出现乱码了。最终爬取的所有网页无论何种编码格式，都转化为utf-8格式进行存储。</p>
<p>2.乱码的解决方法<br>解决一： 获取到response之后直接指定res.encoding = “utf-8”</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">示例：</span><br><span class="line">import requests</span><br><span class="line"></span><br><span class="line">url = &quot;http://search.51job.com&quot;</span><br><span class="line">res = requests.get(url)</span><br><span class="line">res.encoding = &quot;utf-8&quot;</span><br><span class="line">html = res.text</span><br><span class="line">print(html)</span><br></pre></td></tr></table></figure>

<h2 id="http-https介绍详细完整"><a href="#http-https介绍详细完整" class="headerlink" title="http/https介绍详细完整"></a>http/https介绍详细完整</h2><p><a href="https://www.cnblogs.com/woai3c/p/12902325.html" target="_blank" rel="noopener">https://www.cnblogs.com/woai3c/p/12902325.html</a></p>
<h3 id="认识http-https"><a href="#认识http-https" class="headerlink" title="认识http,https"></a>认识http,https</h3><ul>
<li>http: 超文本传输协议<ul>
<li>以明文形式传输</li>
<li>效率更高，但是不安全</li>
</ul>
</li>
<li>https: http + ssl(TCP的加强版，称为安全套接字层（Secure Sockets Layer，SSL）)<ul>
<li>传输之前数据先加密，之后解密获取内容</li>
<li>效率较低，但是安全</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>http协议之请求</p>
<ul>
<li>1.请求行 get … http/1.1</li>
<li>2.请求头：<ul>
<li>host:域名</li>
<li>connection：keep-alive 客户端告诉服务端我们支持使用长连接，不频繁的建立、断开连接。只在最开始建立，最后断开。(服务器在发送响应后保持该TCP连接打开，后续的请求和响应报文能通过相同的连接进行发送。)只需要进行一次 TCP 连接就能进行多次 HTTP 通信。HTTP/1.1开始，所有的连接默认都是持久连接。</li>
<li>cache-control: max-age=0 缓存控制。客户端告诉服务端不做任何缓存</li>
<li>user-agent: 浏览器的身份标识。对方服务器知道你是什么浏览器在请求服务器。</li>
<li>cookie: 保存浏览器本地的用户信息。访问网站带上cookie。如果要请求登录之后才能访问的网站就要处理cookie.</li>
</ul>
</li>
<li>3.请求体</li>
</ul>
</li>
<li><p>http协议之响应</p>
<ul>
<li>1.响应头<ul>
<li>set-cookie: 对方服务器通过该字段设置cookie到本地</li>
</ul>
</li>
<li>2.响应体<ul>
<li>url地址对应的响应</li>
</ul>
</li>
</ul>
</li>
</ul>
<hr>
<ul>
<li><p>爬虫是模仿浏览器去发送请求</p>
</li>
<li><p>抓包：看浏览器发送了什么请求</p>
</li>
<li><p>get和post请求的区别</p>
<ul>
<li>get请求没有请求体，post有，get请求吧数据放到url地址中</li>
<li>post请求常用于登录注册/传输大文本</li>
</ul>
</li>
</ul>
<h3 id="Cookie和Session"><a href="#Cookie和Session" class="headerlink" title="Cookie和Session"></a>Cookie和Session</h3><ol>
<li><p>两个数据存放在哪里？<br>cookie数据存放在客户的浏览器上；session数据放在服务器上。</p>
</li>
<li><p>cookie的特点和限制是什么？</p>
</li>
</ol>
<ul>
<li>cookie不是很安全，因为别人可以分析存放在本地的COOKIE并进行COOKIE欺骗。</li>
<li>单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie。</li>
</ul>
<ol start="3">
<li><p>两个数据里能存什么？<br>cookie里只能存放字符串；session里能存放任何类型的数据，如字符串、集合、类、对象。</p>
</li>
<li><p>用户能看见吗？<br>cookie对用户可见；session对用户不可见(透明)</p>
</li>
<li><p>有效期的差别？ 比如浏览器上百度账号登录信息长期有效？<br>cookie可以容易的达到登录信息长期有效，只要把有效期设为很大的数字；<br>session不能长期有效，容易导致服务器的内存溢出。比如说我们项目使用jwt设置了有效期7天。</p>
</li>
<li><p>并发访问的用户多的时候，用哪个来追踪用户会话？<br>用cookie。<br>因为一个用户就会产生一个session/cookie. session存在服务器端，产生的session量过大会消耗大量的内存。 cookie存在客户端，不占用服务器端的资源，一个用户手里拿一个cookie。<br>所以像百度这种只能用cookie来存储用户的登录信息，用cookie来追踪用户会话。</p>
</li>
<li><p>为什么百度贴吧、百度云盘、百度搜索都可以共用我的登录信息呢(不用重复登录)？<br>因为cookie支持跨域名访问，以“.biaodianfu.com”为后缀的一切域名均能够访问该Cookie。<br>但是session就不支持跨域名访问，如果用session,都要重新登录。</p>
</li>
<li><p>session在服务器端由谁来管理？存在哪里？<br>session由shiro来管理，shiro六大作用之一就是Session会话管理。在ShiroConfig(shiro配置类)中就有会话管理对应的函数-SessionManager.</p>
</li>
<li><p>现在用cookie和session吗？<br>前后端没分离的时候用cookie和session来验证用户的真实性；<br>现在前后端分离了，我们使用shiro这个安全框架来规定用户的操作，使用token和jwt来验证用户的真实性。</p>
</li>
</ol>
<h3 id="post-有道翻译"><a href="#post-有道翻译" class="headerlink" title="post-有道翻译"></a>post-有道翻译</h3><h4 id="网页检查"><a href="#网页检查" class="headerlink" title="网页检查"></a>网页检查</h4><p>1.动态页面用networking-xhr-headers中找<br>Request URL: <a href="http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule" target="_blank" rel="noopener">http://fanyi.youdao.com/translate_o?smartresult=dict&amp;smartresult=rule</a></p>
<p>Form Data(post的表单)：<br>i: 重复<br>from: AUTO<br>to: AUTO<br>smartresult: dict<br>client: fanyideskweb<br>salt: 16121560255974<br>sign: c94d4735db490259191c1678221aadc1<br>lts: 1612156025597<br>bv: e915c77f633538e8cf44c657fe201ebb<br>doctype: json<br>version: 2.1<br>keyfrom: fanyi.web<br>action: FY_BY_REALTlME</p>
<p>2.用networking-xhr-preview/response里找post返回的字典中的翻译结果在哪里。</p>
<h4 id="程序步奏"><a href="#程序步奏" class="headerlink" title="程序步奏"></a>程序步奏</h4><p>1.用控制台输入待翻译词语<br>2.设定待请求的url<br>3.建立post表单(字典形式)<br>4.发起post请求<br>5.接收响应结果，解析提取<br>6.打印出翻译的结果</p>
<h4 id="程序代码"><a href="#程序代码" class="headerlink" title="程序代码"></a>程序代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line">import requests</span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line">while True:</span><br><span class="line">    content = input(&quot;输入要翻译的词语:&quot;)</span><br><span class="line">    # 退出机制-无字符空字符</span><br><span class="line">    if content == &quot;&quot;:</span><br><span class="line">        print(&quot;无有效输入，退出。&quot;)</span><br><span class="line">        break</span><br><span class="line">    url = &quot;http://fanyi.youdao.com/translate?smartresult=dict&amp;smartresult=rule&quot; # 网站反扒措施translate_o，改成translate</span><br><span class="line">    post_form = &#123;&apos;i&apos;: content,</span><br><span class="line">                &apos;from&apos;: &apos;AUTO&apos;,</span><br><span class="line">                &apos;to&apos;: &apos;AUTO&apos;,</span><br><span class="line">                &apos;smartresult&apos;: &apos;dict&apos;,</span><br><span class="line">                &apos;client&apos;: &apos;fanyideskweb&apos;,</span><br><span class="line">                &apos;salt&apos;: &apos;16121560255974&apos;,</span><br><span class="line">                &apos;sign&apos;: &apos;c94d4735db490259191c1678221aadc1&apos;,</span><br><span class="line">                &apos;lts&apos;: &apos;1612156025597&apos;,</span><br><span class="line">                &apos;bv&apos;: &apos;e915c77f633538e8cf44c657fe201ebb&apos;,</span><br><span class="line">                &apos;doctype&apos;: &apos;json&apos;,</span><br><span class="line">                &apos;version&apos;: &apos;2.1&apos;,</span><br><span class="line">                &apos;keyfrom&apos;: &apos;fanyi.web&apos;,</span><br><span class="line">                &apos;action&apos;: &apos;FY_BY_REALTlME&apos;</span><br><span class="line">                &#125;</span><br><span class="line">    # 提交post请求后会返回响应，然后解析响应的text属性为json</span><br><span class="line">    response = requests.post(url,data=post_form)</span><br><span class="line">    response_json = response.text</span><br><span class="line">    # json变成dict对象才能提取里面的东西.</span><br><span class="line">    # 网页的东西&#123;translateResult: [[&#123;tgt: &quot;Have a hair&quot;, src: &quot;发发发&quot;&#125;]], errorCode: 0, type: &quot;zh-CHS2en&quot;,…&#125;从里面提取tgt</span><br><span class="line">    res_dict = json.loads(response_json)</span><br><span class="line">    result = res_dict[&quot;translateResult&quot;][0][0][&quot;tgt&quot;]</span><br><span class="line">    print(result)</span><br></pre></td></tr></table></figure>

<h3 id="豆瓣读书"><a href="#豆瓣读书" class="headerlink" title="豆瓣读书"></a>豆瓣读书</h3><h4 id="网址分析"><a href="#网址分析" class="headerlink" title="网址分析"></a>网址分析</h4><p>豆瓣读书-小说页面-分页页面<br><a href="https://book.douban.com/tag/小说?start=80&amp;type=T" target="_blank" rel="noopener">https://book.douban.com/tag/小说?start=80&amp;type=T</a></p>
<p>分析分页链接url的规律<br>MIN start=0<br>MAX start=980<br>一页 start += 20</p>
<h5 id="中文字符串-gt-ASCII码"><a href="#中文字符串-gt-ASCII码" class="headerlink" title="中文字符串-&gt;ASCII码"></a>中文字符串-&gt;ASCII码</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line"># url复制过来是中文变成ascii码 https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4?start=20&amp;type=T</span><br><span class="line"># urllib.request库中的quote函数将中文字符串转换成ascii码</span><br><span class="line">key1 = &quot;小说&quot;</span><br><span class="line">key2 = &quot;novel&quot;</span><br><span class="line">key_ASCII1 = urllib.request.quote(key1)</span><br><span class="line">key_ASCII2 = urllib.request.quote(key2)</span><br><span class="line">print(key_ASCII1)</span><br><span class="line">print(key_ASCII2)</span><br></pre></td></tr></table></figure>

<h5 id="爬取小说页面所有分页html-py"><a href="#爬取小说页面所有分页html-py" class="headerlink" title="爬取小说页面所有分页html.py"></a>爬取小说页面所有分页html.py</h5><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import requests</span><br><span class="line"># url复制过来是中文变成ascii码 https://book.douban.com/tag/%E5%B0%8F%E8%AF%B4?start=20&amp;type=T</span><br><span class="line"># urllib.request库中的quote函数将中文字符串转换成ascii码</span><br><span class="line">key1 = &quot;小说&quot;</span><br><span class="line">key_ASCII1 = urllib.request.quote(key1)</span><br><span class="line">url = &quot;https://book.douban.com/tag/&quot; + key_ASCII1 + &quot;?start=&quot; + str(0) + &quot;&amp;type=T&quot;</span><br><span class="line"># 换一种for range()</span><br><span class="line">#  requests.get(url,headers=headers) 用User-Agent做headers伪装浏览器才能爬取到，不然爬下来的打开空白 .这样做很快</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&quot;</span><br><span class="line">&#125;</span><br><span class="line">for i in range(0,50):</span><br><span class="line">    paging_url = &quot;https://book.douban.com/tag/&quot; + key_ASCII1 + &quot;?start=&quot; + str(i*20) + &quot;&amp;type=T&quot;</span><br><span class="line">    response = requests.get(paging_url,headers=headers)</span><br><span class="line">    data = response.text</span><br><span class="line">    file_path = &quot;D:\材料-公众号-爬虫小说\py爬虫-豆瓣读书\小说分页内容\第&quot; + str(i) + &quot;页.html&quot;</span><br><span class="line">    with open(file_path, &quot;w&quot;, encoding=&quot;utf-8&quot;) as f:</span><br><span class="line">        f.write(data)</span><br></pre></td></tr></table></figure>

<h4 id="使用BeautifulSoup-css选择器从一整个html中提取内容"><a href="#使用BeautifulSoup-css选择器从一整个html中提取内容" class="headerlink" title="使用BeautifulSoup+css选择器从一整个html中提取内容"></a>使用BeautifulSoup+css选择器从一整个html中提取内容</h4><ul>
<li><p>和之前爬虫小说的区别在于：<br>爬虫小说直接根据css选择器拿到所有的链接/文本再删减。 (css选择器用的是后代的规律)<br>使用BeautifulSoup也是用css选择器拿，只不过拿所有的方法更加精确(使用了循环，css选择器是整个)。</p>
</li>
<li><p>步奏：<br>为了拿到书名</p>
</li>
</ul>
<p>1.拿到一个分页页面的html<br>2.用BeautifulSoup解析网页<br>3.用css选择器拿到内容标签对象(根据css选择器的规律做循环提取) </p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">&lt;class &apos;bs4.element.ResultSet&apos;&gt;</span><br></pre></td></tr></table></figure>

<p>3.2 “#subject_list &gt; ul &gt; li:nth-child(1) &gt; div.info &gt; h2 &gt; a” “#subject_list &gt; ul &gt; li:nth-child(20) &gt; div.info &gt; h2 &gt; a”规律:nth-child从1到20<br>3.3 字符串中的变量占位两种： 1.f” {i} “  2. “ “+str(i)+” “<br>4.从提取标签中的text文本<br>5.根据文本情况处理掉换行和空格<br>bug: 有些页面的选择器规律是1到19.这样到20的时候就会报错。<br>解决1：用try except解决。让报错跳过。<br>解决2：报错是因为取list时候的[0].没有这个list就取不到。用for in 循环取(虽然最多只有一个元素)</p>
<ul>
<li>代码<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">for i in range(0,50):</span><br><span class="line">    paging_url = &quot;https://book.douban.com/tag/&quot; + key_ASCII1 + &quot;?start=&quot; + str(i*20) + &quot;&amp;type=T&quot;</span><br><span class="line">    response = requests.get(paging_url,headers=headers)</span><br><span class="line">    data = response.text</span><br><span class="line">    # BeautifulSoup(响应数据,&quot;html.parse&quot;解析网页的意思)</span><br><span class="line">    soup = BeautifulSoup(data,&quot;html.parser&quot;)</span><br><span class="line">    try: # 解决1</span><br><span class="line">        for i in range(1,21): # 从1到20</span><br><span class="line">            # a = soup.select(f&quot;#subject_list &gt; ul &gt; li:nth-child(&#123;i&#125;) &gt; div.info &gt; h2 &gt; a&quot;) # 用a标签拿到的是列表，要把里面的标签内容取出来</span><br><span class="line">            a = soup.select(&quot;#subject_list &gt; ul &gt; li:nth-child(&quot;+str(i)+&quot;) &gt; div.info &gt; h2 &gt; a&quot;)</span><br><span class="line">            # print(type(a)) # &lt;class &apos;bs4.element.ResultSet&apos;&gt;</span><br><span class="line">            #a_list_0 = a[0] # 取出的是一个a标签整体</span><br><span class="line">            for one in a: # 解决2</span><br><span class="line">                book_name = one.get_text() # 取出的是a标签的内容(文字)，里面有很多换行和空格，需要去掉</span><br><span class="line">                book_name = book_name.replace(&quot;\n&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;) # 换行和空格 替换成 没有&quot;&quot;</span><br><span class="line">                print(book_name)</span><br><span class="line">    except:</span><br><span class="line">        pass</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="完整版代码"><a href="#完整版代码" class="headerlink" title="完整版代码"></a>完整版代码</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br></pre></td><td class="code"><pre><span class="line">import urllib.request</span><br><span class="line">import requests</span><br><span class="line">from bs4 import BeautifulSoup</span><br><span class="line">import csv</span><br><span class="line">key1 = &quot;小说&quot;</span><br><span class="line">key2 = &quot;novel&quot;</span><br><span class="line">key_ASCII1 = urllib.request.quote(key1)</span><br><span class="line">key_ASCII2 = urllib.request.quote(key2)</span><br><span class="line">print(key_ASCII1)</span><br><span class="line">print(key_ASCII2)</span><br><span class="line"></span><br><span class="line"># 用for循环把中文list-&gt;ASCII list</span><br><span class="line">key_list = [&quot;小说&quot;,&quot;散文&quot;,&quot;诗歌&quot;]</span><br><span class="line">key_ASCII_list = []</span><br><span class="line">for key in key_list:</span><br><span class="line">    key_ASCII_list.append(urllib.request.quote(key))</span><br><span class="line"># print(key_ASCII_list)</span><br><span class="line"></span><br><span class="line"># url使用字符串拼接(全部用str()转换)</span><br><span class="line">url = &quot;https://book.douban.com/tag/&quot; + key_ASCII1 + &quot;?start=&quot; + str(0) + &quot;&amp;type=T&quot;</span><br><span class="line"></span><br><span class="line"># 换一种for range()</span><br><span class="line">#  requests.get(url,headers=headers) 用User-Agent做headers伪装浏览器才能爬取到，不然爬下来的打开空白 .这样做很快</span><br><span class="line">headers = &#123;</span><br><span class="line">    &quot;User-Agent&quot;: &quot;Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/85.0.4183.83 Safari/537.36&quot;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"># 为了拿到书名</span><br><span class="line"># 1.拿到一个分页页面的html</span><br><span class="line"># 2.用BeautifulSoup解析网页</span><br><span class="line"># 3.用css选择器拿到内容标签对象(根据css选择器的规律做循环提取) &lt;class &apos;bs4.element.ResultSet&apos;&gt;</span><br><span class="line"># 3.2 &quot;#subject_list &gt; ul &gt; li:nth-child(1) &gt; div.info &gt; h2 &gt; a&quot; &quot;#subject_list &gt; ul &gt; li:nth-child(20) &gt; div.info &gt; h2 &gt; a&quot;规律:nth-child从1到20</span><br><span class="line"># 3.3 字符串中的变量占位两种： 1.f&quot; &#123;i&#125; &quot;  2. &quot; &quot;+str(i)+&quot; &quot;</span><br><span class="line"># 4.从提取标签中的text文本</span><br><span class="line"># 5.根据文本情况处理掉换行和空格</span><br><span class="line"># bug: 有些页面的选择器规律是1到19.这样到20的时候就会报错。</span><br><span class="line"># 解决1：用try except解决。让报错跳过。</span><br><span class="line"># 解决2：报错是因为取list时候的[0].没有这个list就取不到。用for in 循环取(虽然最多只有一个元素)</span><br><span class="line">book_name_list = []</span><br><span class="line">author_list = []</span><br><span class="line">publisher_list = []</span><br><span class="line">publish_time_list = []</span><br><span class="line">price_list = []</span><br><span class="line">score_list = []</span><br><span class="line">evaluation_list = []</span><br><span class="line">for i in range(0,50): # (0,50)</span><br><span class="line">    paging_url = &quot;https://book.douban.com/tag/&quot; + key_ASCII1 + &quot;?start=&quot; + str(i*20) + &quot;&amp;type=T&quot;</span><br><span class="line">    response = requests.get(paging_url,headers=headers)</span><br><span class="line">    data = response.text</span><br><span class="line">    # BeautifulSoup(响应数据,&quot;html.parse&quot;解析网页的意思)</span><br><span class="line">    soup = BeautifulSoup(data,&quot;html.parser&quot;)</span><br><span class="line">    try: # 解决1</span><br><span class="line">        for i in range(1,21): # 从1到20 (1,21)</span><br><span class="line">            # 1.拿书名(第一行)</span><br><span class="line">            a = soup.select(f&quot;#subject_list &gt; ul &gt; li:nth-child(&#123;i&#125;) &gt; div.info &gt; h2 &gt; a&quot;) # 用a标签拿到的是列表，要把里面的标签内容取出来</span><br><span class="line">            #a = soup.select(&quot;#subject_list &gt; ul &gt; li:nth-child(&quot;+str(i)+&quot;) &gt; div.info &gt; h2 &gt; a&quot;)</span><br><span class="line">            # print(type(a)) # &lt;class &apos;bs4.element.ResultSet&apos;&gt;</span><br><span class="line">            #a_list_0 = a[0] # 取出的是一个a标签整体</span><br><span class="line">            for one in a: # 解决2:把东西从列表中取出后，才能取出标签中的文字</span><br><span class="line">                book_name = one.get_text() # 取出的是a标签的内容(文字)，里面有很多换行和空格，需要去掉</span><br><span class="line">                book_name = book_name.replace(&quot;\n&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;) # 换行和空格 替换成 没有&quot;&quot;</span><br><span class="line">                # print(book_name)</span><br><span class="line">                book_name_list.append(book_name)</span><br><span class="line"></span><br><span class="line">            # 2.拿作者...(第二行)</span><br><span class="line">            book_info = soup.select(f&quot;#subject_list &gt; ul &gt; li:nth-child(&#123;i&#125;) &gt; div.info &gt; div.pub&quot;)</span><br><span class="line">            for one in book_info:</span><br><span class="line">                book_info = one.get_text()</span><br><span class="line">                book_info = book_info.replace(&quot;\n&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;)</span><br><span class="line">                author_time_price_list = book_info.split(&quot;/&quot;) # 余华/作家出版社/2012-8-1/20.00元. [日]东野圭吾/刘姿君/南海出版公司/2013-1-1/39.50元</span><br><span class="line">                # 正取作者，反取出版社，时间，价格. 如果元素少于4个，手动增加元素</span><br><span class="line">                if len(author_time_price_list) &gt;=4:</span><br><span class="line">                    author = author_time_price_list[0]</span><br><span class="line">                    publisher = author_time_price_list[-3]</span><br><span class="line">                    publish_time = author_time_price_list[-2]</span><br><span class="line">                    price = author_time_price_list[-1].replace(&quot;元&quot;,&quot;&quot;).replace(&quot;CNY&quot;,&quot;&quot;)</span><br><span class="line">                else:</span><br><span class="line">                    author = author_time_price_list[0]</span><br><span class="line">                    publisher = &quot;null&quot;</span><br><span class="line">                    publish_time = &quot;null&quot;</span><br><span class="line">                    price = &quot;null&quot;</span><br><span class="line"></span><br><span class="line">                author_list.append(author)</span><br><span class="line">                publisher_list.append(publisher)</span><br><span class="line">                publish_time_list.append(publish_time)</span><br><span class="line">                price_list.append(price)</span><br><span class="line">                # print(author_time_price_list,author_list,publisher_list,publish_time_list,price_list)</span><br><span class="line"></span><br><span class="line">            # 3.拿评价(第三行)</span><br><span class="line">            score = soup.select(f&quot;#subject_list &gt; ul &gt; li:nth-child(&#123;i&#125;) &gt; div.info &gt; div.star.clearfix &gt; span.rating_nums&quot;)</span><br><span class="line">            for one in score:</span><br><span class="line">                score = one.get_text()</span><br><span class="line">                score = score.replace(&quot;\n&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;)</span><br><span class="line">                score_list.append(score)</span><br><span class="line">            # print(score_list)</span><br><span class="line"></span><br><span class="line">            evaluation = soup.select(f&quot;#subject_list &gt; ul &gt; li:nth-child(&#123;i&#125;) &gt; div.info &gt; div.star.clearfix &gt; span.pl&quot;)</span><br><span class="line">            for one in evaluation:</span><br><span class="line">                evaluation = one.get_text()</span><br><span class="line">                evaluation = score.replace(&quot;\n&quot;,&quot;&quot;).replace(&quot; &quot;,&quot;&quot;).replace(&quot;(&quot;,&quot;&quot;).replace(&quot;人评价)&quot;,&quot;&quot;)</span><br><span class="line">                evaluation_list.append(evaluation)</span><br><span class="line">            # print(evaluation_list)</span><br><span class="line"></span><br><span class="line">    except:</span><br><span class="line">        pass</span><br><span class="line"></span><br><span class="line"># 保证每个list的信息一一对应</span><br><span class="line">print(len(book_name_list))</span><br><span class="line">print(len(author_list))</span><br><span class="line">print(len(publisher_list))</span><br><span class="line">print(len(publish_time_list))</span><br><span class="line">print(len(price_list))</span><br><span class="line">print(len(score_list))</span><br><span class="line">print(len(evaluation_list))</span><br><span class="line"></span><br><span class="line"># 以字典方式写入csv</span><br><span class="line">csv_file_path = &quot;D:\材料-公众号-爬虫小说\py爬虫-豆瓣读书\csv文件\豆瓣小说分页.csv&quot;</span><br><span class="line">with open(csv_file_path,&quot;w&quot;,newline=&quot;&quot;,encoding=&quot;utf-8-sig&quot;) as f: # 不空行，中文不乱码</span><br><span class="line">    field_dict_name = [&quot;书名&quot;,&quot;作者&quot;,&quot;出版社&quot;,&quot;出版时间&quot;,&quot;价格(元)&quot;,&quot;评分&quot;, &quot;评价&quot;] # csv的表头</span><br><span class="line">    f_csv = csv.DictWriter(f,fieldnames=field_dict_name) # 文件变量</span><br><span class="line">    f_csv.writeheader() # 写表头</span><br><span class="line">    # 写入每一行</span><br><span class="line">    for i in range(0, len(book_name_list)):</span><br><span class="line">        f_csv.writerow(</span><br><span class="line">            &#123;</span><br><span class="line">                &quot;书名&quot;: book_name_list[i],</span><br><span class="line">                &quot;作者&quot;: author_list[i],</span><br><span class="line">                &quot;出版社&quot;: publisher_list[i],</span><br><span class="line">                &quot;出版时间&quot;: publish_time_list[i],</span><br><span class="line">                &quot;价格(元)&quot;: price_list[i],</span><br><span class="line">                &quot;评分&quot;: score_list[i],</span><br><span class="line">                &quot;评价&quot;: evaluation_list[i]</span><br><span class="line">            &#125;</span><br><span class="line">        )</span><br></pre></td></tr></table></figure>
    </div>

    
    
    
        
      
        <div id="reward-container">
  <div>我这么可爱不给我投个币嘛~~ 有帮助的话可以打赏路路噢~~</div>
  <button id="reward-button" disable="enable" onclick="var qr = document.getElementById(&quot;qr&quot;); qr.style.display = (qr.style.display === 'none') ? 'block' : 'none';">
    打赏
  </button>
  <div id="qr" style="display: none;">
        
      
      <div style="display: inline-block">
        <img src="/images/wechatpay.jpg" alt="Lucifinil 微信支付">
        <p>微信支付</p>
      </div>
        
      
      <div style="display: inline-block">
        <img src="/images/alipay.jpg" alt="Lucifinil 支付宝">
        <p>支付宝</p>
      </div>

  </div>
</div>

      

      <footer class="post-footer">

        

          <div class="post-nav">
            <div class="post-nav-next post-nav-item">
              
                <a href="/2020/08/31/css3/" rel="next" title="css3">
                  <i class="fa fa-chevron-left"></i> css3
                </a>
              
            </div>

            <span class="post-nav-divider"></span>

            <div class="post-nav-prev post-nav-item">
              
                <a href="/2020/09/07/INT201-决策计算与语言/" rel="prev" title="INT201 决策计算与语言(编译原理)">
                  INT201 决策计算与语言(编译原理) <i class="fa fa-chevron-right"></i>
                </a>
              
            </div>
          </div>
        
      </footer>
    
  </div>
  
  
  
  </article>

  </div>


          </div>
          
    
    <div class="comments" id="comments"></div>
  

        </div>
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">
        
        
        
        
      

      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
          <div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-3"><a class="nav-link" href="#爬虫是什么"><span class="nav-number">1.</span> <span class="nav-text">爬虫是什么</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#编译器安装"><span class="nav-number">2.</span> <span class="nav-text">编译器安装</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用css选择器爬一章小说"><span class="nav-number">3.</span> <span class="nav-text">用css选择器爬一章小说</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#需要的函数库"><span class="nav-number">3.1.</span> <span class="nav-text">需要的函数库</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#封装爬一本小说一章节的方法"><span class="nav-number">3.2.</span> <span class="nav-text">封装爬一本小说一章节的方法</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ip池"><span class="nav-number">3.3.</span> <span class="nav-text">ip池</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬一整本-反叛-问题"><span class="nav-number">3.4.</span> <span class="nav-text">爬一整本(反叛)问题</span></a></li></ol></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#下面这个进行强制类型转换成列表，去掉前面两个"><span class="nav-number"></span> <span class="nav-text">下面这个进行强制类型转换成列表，去掉前面两个</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#爬全站"><span class="nav-number">0.1.</span> <span class="nav-text">爬全站</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#爬一本练习-被爱幻想"><span class="nav-number">0.2.</span> <span class="nav-text">爬一本练习(被爱幻想)</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用css选择器爬囚徒-有分页，自己分段"><span class="nav-number">1.</span> <span class="nav-text">用css选择器爬囚徒(有分页，自己分段)</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用xpath爬狱警与黑老大"><span class="nav-number">2.</span> <span class="nav-text">用xpath爬狱警与黑老大</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用css选择器爬御宅屋"><span class="nav-number">3.</span> <span class="nav-text">用css选择器爬御宅屋</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#做两次跳转的函数"><span class="nav-number">3.1.</span> <span class="nav-text">做两次跳转的函数</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#用-text-抓取下来的文字乱码"><span class="nav-number">4.</span> <span class="nav-text">用.text()抓取下来的文字乱码</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#http-https介绍详细完整"><span class="nav-number"></span> <span class="nav-text">http/https介绍详细完整</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#认识http-https"><span class="nav-number">1.</span> <span class="nav-text">认识http,https</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#Cookie和Session"><span class="nav-number">2.</span> <span class="nav-text">Cookie和Session</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#post-有道翻译"><span class="nav-number">3.</span> <span class="nav-text">post-有道翻译</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网页检查"><span class="nav-number">3.1.</span> <span class="nav-text">网页检查</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#程序步奏"><span class="nav-number">3.2.</span> <span class="nav-text">程序步奏</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#程序代码"><span class="nav-number">3.3.</span> <span class="nav-text">程序代码</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#豆瓣读书"><span class="nav-number">4.</span> <span class="nav-text">豆瓣读书</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#网址分析"><span class="nav-number">4.1.</span> <span class="nav-text">网址分析</span></a><ol class="nav-child"><li class="nav-item nav-level-5"><a class="nav-link" href="#中文字符串-gt-ASCII码"><span class="nav-number">4.1.1.</span> <span class="nav-text">中文字符串-&gt;ASCII码</span></a></li><li class="nav-item nav-level-5"><a class="nav-link" href="#爬取小说页面所有分页html-py"><span class="nav-number">4.1.2.</span> <span class="nav-text">爬取小说页面所有分页html.py</span></a></li></ol></li><li class="nav-item nav-level-4"><a class="nav-link" href="#使用BeautifulSoup-css选择器从一整个html中提取内容"><span class="nav-number">4.2.</span> <span class="nav-text">使用BeautifulSoup+css选择器从一整个html中提取内容</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#完整版代码"><span class="nav-number">4.3.</span> <span class="nav-text">完整版代码</span></a></li></ol></li></ol></div>
        
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
    <img class="site-author-image" itemprop="image"
      src="/images/touxiang1.jpg"
      alt="Lucifinil">
  <p class="site-author-name" itemprop="name">Lucifinil</p>
  <div class="site-description" itemprop="description"></div>
</div>
  <nav class="site-state motion-element">
      <div class="site-state-item site-state-posts">
        
          <a href="/archives/">
        
          <span class="site-state-item-count">66</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-categories">
        
          
            <a href="/categories/">
          
        
        <span class="site-state-item-count">9</span>
        <span class="site-state-item-name">分类</span>
        </a>
      </div>
    
      
      
      <div class="site-state-item site-state-tags">
        
          
            <a href="/tags/">
          
        
        <span class="site-state-item-count">5</span>
        <span class="site-state-item-name">标签</span>
        </a>
      </div>
    
  </nav>
  <div class="links-of-author motion-element">
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://github.com/Lucifinil-x" title="GitHub &rarr; https://github.com/Lucifinil-x" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i>GitHub</a>
      </span>
    
      <span class="links-of-author-item">
      
      
        
      
      
        
      
        <a href="https://weibo.com/2890499315/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" title="weibo &rarr; https://weibo.com/2890499315/profile?rightmod=1&wvr=6&mod=personinfo&is_all=1" rel="noopener" target="_blank"><i class="fa fa-fw fa-weibo"></i>weibo</a>
      </span>
    
  </div>


  <div class="links-of-blogroll motion-element">
    <div class="links-of-blogroll-title">
      <i class="fa fa-fw fa-link"></i>
      Links
    </div>
    <ul class="links-of-blogroll-list">
        <li class="links-of-blogroll-item">
          <a href="http://Lucifinil.com/" title="http://Lucifinil.com/" rel="noopener" target="_blank">Lucifinil</a>
        </li>
      
    </ul>
  </div>

      </div>
        <div class="back-to-top motion-element">
          <i class="fa fa-arrow-up"></i>
          <span>0%</span>
        </div>
      

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Lucifinil</span>
</div>

<!--
  <div class="powered-by">由 <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> 强力驱动 v3.9.0</div>
  <span class="post-meta-divider">|</span>
  <div class="theme-info">主题 – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Pisces</a> v7.4.0</div>-->

        
<div class="busuanzi-count">
  <script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>
    <span class="post-meta-item-icon">
      <i class="fa fa-user"></i>
    </span>
    <span class="site-uv" title="总访客量">
      <span class="busuanzi-value" id="busuanzi_value_site_uv"></span>
    </span>
  
    <span class="post-meta-divider">|</span>
  
    <span class="post-meta-item-icon">
      <i class="fa fa-eye"></i>
    </span>
    <span class="site-pv" title="总访问量">
      <span class="busuanzi-value" id="busuanzi_value_site_pv"></span>
    </span>
  
</div>












        
      </div>
    </footer>
  </div>

  


  <script src="/lib/anime.min.js?v=3.1.0"></script>
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
<script src="/js/utils.js?v=7.4.0"></script><script src="/js/motion.js?v=7.4.0"></script>
<script src="/js/schemes/pisces.js?v=7.4.0"></script>
<script src="/js/next-boot.js?v=7.4.0"></script>

    
  
    
  

  <script>
  (function() {
    window.addEventListener('DOMContentLoaded', () => {
      let script = document.createElement('script');
      script.src = '/lib/three/three.min.js';
      document.body.appendChild(script);
    });
    let styles = ['/lib/three/three-waves.min.js', '', '/lib/three/canvas_sphere.min.js'];
    window.addEventListener('load', () => {
      styles.forEach(item => {
        if (item !== '') {
          let script = document.createElement('script');
          script.src = item;
          document.body.appendChild(script);
        }
      });
    });
  })();
  </script>


  








  <script src="/js/local-search.js?v=7.4.0"></script>














  

  

  


<script>
NexT.utils.getScript('//unpkg.com/valine/dist/Valine.min.js', () => {
  var GUEST = ['nick', 'mail', 'link'];
  var guest = 'nick,mail,link';
  guest = guest.split(',').filter(item => {
    return GUEST.includes(item);
  });
  new Valine({
    el: '#comments',
    verify: false,
    notify: false,
    appId: 'oDY3PgVcs4KquzU5ExfpRIvL-gzGzoHsz',
    appKey: 'TfeAqOOA3dvfY8DgjnBCdFUI',
    placeholder: 'Just go go',
    avatar: 'mm',
    meta: guest,
    pageSize: '10' || 10,
    visitor: false,
    lang: '' || 'zh-cn',
    path: location.pathname
  });
}, window.Valine);
</script>

</body>
</html>
<!-- 页面点击小红心 -->
<script type="text/javascript" src="/js/src/clicklove.js"></script>